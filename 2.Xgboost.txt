xgboost hyperparameters
1.l2 ( lambda)  - 100 (larger the value, more conservative)
used l2 because dont want to delete any feature and also prevent overfitting
(l1(alpha) creates sparse weight matrix which is not required here,)
2. booster - gbtree (not gblinear)
3. tree_method = (auto,exact, approx, hist)
auto: Same as the hist tree method.
exact: Exact greedy algorithm. Enumerates all split candidates.
approx: Approximate greedy algorithm using quantile sketch and gradient histogram. for each tree, different quantile sketchs will be used, so it is slow
hist: Faster histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching. It builds a global histogram before training and refines it for each node during tree construction. its faster

4. learning rate - eta - 0.1
5. subsample - 0.9
(to prevent overfitting)
6. max_depth - 5
(to prevent overfitting)
7. min_child_weight = 2
8. gamma - min loss required to make a split - 0.2 (A node is split only when the resulting split gives a positive reduction in the loss function.)

{'subsample': 0.9,
 'reg_alpha': 100,
 'min_child_weight': 2,
 'max_depth': 5,
 'gamma': 0.2,
 'eta': 0.1,
 'colsample_bytree': 1}
-------------------train-test-split ---------------------------------------------
from sklearn.model_selection import TimeSeriesSplit
tss = TimeSeriesSplit(n_splits = 5, gap = 24, test_size = 30*24)

eg:
Fold 0:
  Train: index=[0]
  Test:  index=[1]
Fold 1:
  Train: index=[0 1]
  Test:  index=[2]
Fold 2:
  Train: index=[0 1 2]
  Test:  index=[3]
Fold 3:
  Train: index=[0 1 2 3]
  Test:  index=[4]
Fold 4:
  Train: index=[0 1 2 3 4]
  Test:  index=[5]

scores = [23,19,22,24,20]
taken np.mean scores 

we used cross val score to mitigate over-fitting problems


2019(1st jan)-2022(31st dec) total=   35040
data hourly data, on each day 24 data points were collected

before final predictions:
tarin the data on the entire dataset (train + val) (2019 to 2022 31st dec) rnmse - 2.2261
it will predict 2023 (1st jan to 30th jan - 30 days)

after automation and model deployment:
the trigger will run after every 30 days ( when new 30 days data is added), and it will predict next 30 days prediction 
new 30 data added which have temp , all the features like hour, month, quarter etc, also have 24 lags 
the prediction 30 days == 720 data points will have all the lags(previous data is avl, also, next 30 days time is avl. so month, quarter, day is also avl) but no target var(temp)
-------------------- feature creation ------------------------------
df["hour"]
df["day_of_week"] - monday 0 , sunday - 6
df["quarter"]
df["month"]
df["year"]
df["day_of_year"]

creation of lags
def add_lags(df):
    target_map = df["temp"].to_dict()
    df["lag1"] = (df.index - pd.Timedelta('24 h')).map(target_map)
    df["lag2"] = (df.index - pd.Timedelta('48 h')).map(target_map)
    df["lag3"] = (df.index - pd.Timedelta('72 h')).map(target_map)
    df["lag4"] = (df.index - pd.Timedelta('96 h')).map(target_map)
    df["lag5"] = (df.index - pd.Timedelta('120 h')).map(target_map)
    df["lag6"] = (df.index - pd.Timedelta('144 h')).map(target_map)
    df["lag7"] = (df.index - pd.Timedelta('168 h')).map(target_map)
    df["lag8"] = (df.index - pd.Timedelta('192 h')).map(target_map)
    df["lag9"] = (df.index - pd.Timedelta('216 h')).map(target_map)
    return df

every hour of a dayhas a lag1 of 24 hrs of exaxt same timestamp of the previous day and so on.

------------------- metric------------------
from skelarn.metric import mean_squared_error
mse will give more penalty for the prdcictions that are way off versus just a little way off , also unit is not same. so going with rmse

self defined metric - MAE% = ( avg error / avg temp) unline MAPE ( avg( error / temp )) 

Probelms in MAPE 
MAPE divides each error individually by the temp, so it is skewed: high errors during low-temp periods will significantly impact MAPE. Due to this, 
optimizing MAPE will result in a strange forecast that will most likely undershoot the temp. Just avoid it.

Problems in MSE
Many algorithms use MSE as it is faster to compute and easier to manipulate than RMSE. But it is not scaled to the original error (as the error is squared), 
resulting in a KPI that we cannot relate to the original temp scale. Therefore, we won’t use it to evaluate our statistical forecast models.

**************MAE protects outliers, whereas RMSE assures us to get an unbiased forecast. MAE aims at the median, MSE aims at the average.
----------------------------------outliers--------------------------------------------
Q 1 − 1.5 ⋅ IQR ‍ and high outliers are above Q 3 + 1.5 ⋅ IQR ‍

since, the data has very few outliers(when temp is 0 during shut down), (like intermittent demand one value 100, next 2 values 0)  we used RMSE just to ensure unbiased error,  rmse = 2.2261
we deliverately delete the outliers so that model can learn the pattern when the machine will failire after its temp is gradually increasing. 
total  411 points were outliers out of 35000 that means these 411 points have temp 0 (stopping condition) [eg, 2019-05-17 09:00, 10:00, 11:00 values are aero, means 3 hrs shutdown, then again started and temp will be logged accordingly)
 
calculate error
np.abs(test["target'] - test["prediction']

from sklearn.metrics import mean_absolute_percentage_error
print(mean_absolute_percentage_error(y_test, y_pred))  mape = 0.0432

atest.groupby("date)["error"].mean().sort_values(ascending = False).head(5)
checked which date has best predictions, which have worst prediction
------------------------------ cross val score ----------------------
cross val score
[10.544051150621375, 5.819618939173513, 2.7668823779318883, 3.3297714574717885, 3.8800631281756015]

just for safe sided checking mape. but we would use rmse which is 2 for training data , 5 for testing/validation data , so good performance by the model 


mean score = rmse = 5.2681
mape = 0.0432

----------------- model ------------------
import xgboost as xgb
model = xgb.XGBRegressor(n_estimators = 1000, early_Stopping_rounds = 50, )
model.fit(X_train,y_train, ,eval_set = [(X_train,y_train), (X_test, y_test)], verbose = 100)
df["prediction"] = model.predict(X_test]

---------------------------- feature importance----------------------
model.feature_importances
1. hour
2. month
3.day of week
4.day of year
5. quarter
6. year

Gain: Represents the average improvement in the model's objective function (e.g., loss reduction) across all splits where a particular feature was used.
Cover: Represents the proportion of instances passing through a specific feature's splits in the trees.
gain = model.gain
If splitting by area leads to a significant reduction in squared error (objective function) compared to not splitting, the area feature's gain score will be high.
cover = model.cover
If the model frequently splits data based on number of bedrooms across multiple trees, the cover score for bedrooms will be high.



